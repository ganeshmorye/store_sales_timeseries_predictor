{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Handling and Processing\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Machine Learning and Data Preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Utilities and Miscellaneous\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_tbl_width_chars(width= 200)\n",
    "pl.Config.set_tbl_cols(12)\n",
    "pl.Config.set_tbl_rows(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3_000_888, 6)\n",
      "┌─────────┬────────────┬───────────┬────────────────────────────┬──────────┬─────────────┐\n",
      "│ id      ┆ date       ┆ store_nbr ┆ family                     ┆ sales    ┆ onpromotion │\n",
      "│ ---     ┆ ---        ┆ ---       ┆ ---                        ┆ ---      ┆ ---         │\n",
      "│ i64     ┆ date       ┆ str       ┆ str                        ┆ f64      ┆ i64         │\n",
      "╞═════════╪════════════╪═══════════╪════════════════════════════╪══════════╪═════════════╡\n",
      "│ 0       ┆ 2013-01-01 ┆ 1         ┆ AUTOMOTIVE                 ┆ 0.0      ┆ 0           │\n",
      "│ 1       ┆ 2013-01-01 ┆ 1         ┆ BABY CARE                  ┆ 0.0      ┆ 0           │\n",
      "│ 2       ┆ 2013-01-01 ┆ 1         ┆ BEAUTY                     ┆ 0.0      ┆ 0           │\n",
      "│ 3       ┆ 2013-01-01 ┆ 1         ┆ BEVERAGES                  ┆ 0.0      ┆ 0           │\n",
      "│ 4       ┆ 2013-01-01 ┆ 1         ┆ BOOKS                      ┆ 0.0      ┆ 0           │\n",
      "│ …       ┆ …          ┆ …         ┆ …                          ┆ …        ┆ …           │\n",
      "│ 3000883 ┆ 2017-08-15 ┆ 9         ┆ POULTRY                    ┆ 438.133  ┆ 0           │\n",
      "│ 3000884 ┆ 2017-08-15 ┆ 9         ┆ PREPARED FOODS             ┆ 154.553  ┆ 1           │\n",
      "│ 3000885 ┆ 2017-08-15 ┆ 9         ┆ PRODUCE                    ┆ 2419.729 ┆ 148         │\n",
      "│ 3000886 ┆ 2017-08-15 ┆ 9         ┆ SCHOOL AND OFFICE SUPPLIES ┆ 121.0    ┆ 8           │\n",
      "│ 3000887 ┆ 2017-08-15 ┆ 9         ┆ SEAFOOD                    ┆ 16.0     ┆ 0           │\n",
      "└─────────┴────────────┴───────────┴────────────────────────────┴──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "train_df = pl.read_csv('../data/input/train.csv', try_parse_dates=True, dtypes={'store_nbr': str})\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (28_512, 5)\n",
      "┌─────────┬────────────┬───────────┬────────────────────────────┬─────────────┐\n",
      "│ id      ┆ date       ┆ store_nbr ┆ family                     ┆ onpromotion │\n",
      "│ ---     ┆ ---        ┆ ---       ┆ ---                        ┆ ---         │\n",
      "│ i64     ┆ date       ┆ str       ┆ str                        ┆ i64         │\n",
      "╞═════════╪════════════╪═══════════╪════════════════════════════╪═════════════╡\n",
      "│ 3000888 ┆ 2017-08-16 ┆ 1         ┆ AUTOMOTIVE                 ┆ 0           │\n",
      "│ 3000889 ┆ 2017-08-16 ┆ 1         ┆ BABY CARE                  ┆ 0           │\n",
      "│ 3000890 ┆ 2017-08-16 ┆ 1         ┆ BEAUTY                     ┆ 2           │\n",
      "│ 3000891 ┆ 2017-08-16 ┆ 1         ┆ BEVERAGES                  ┆ 20          │\n",
      "│ 3000892 ┆ 2017-08-16 ┆ 1         ┆ BOOKS                      ┆ 0           │\n",
      "│ …       ┆ …          ┆ …         ┆ …                          ┆ …           │\n",
      "│ 3029395 ┆ 2017-08-31 ┆ 9         ┆ POULTRY                    ┆ 1           │\n",
      "│ 3029396 ┆ 2017-08-31 ┆ 9         ┆ PREPARED FOODS             ┆ 0           │\n",
      "│ 3029397 ┆ 2017-08-31 ┆ 9         ┆ PRODUCE                    ┆ 1           │\n",
      "│ 3029398 ┆ 2017-08-31 ┆ 9         ┆ SCHOOL AND OFFICE SUPPLIES ┆ 9           │\n",
      "│ 3029399 ┆ 2017-08-31 ┆ 9         ┆ SEAFOOD                    ┆ 0           │\n",
      "└─────────┴────────────┴───────────┴────────────────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "q= pl.scan_csv('../data/input/test.csv', try_parse_dates=True, dtypes={'store_nbr': str})\n",
    "test_df = q.collect()\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Concatenate train and test dataframes vertically\n",
    "combined_df = pl.concat([train_df, test_df], how='diagonal')\n",
    "\n",
    "\n",
    "# Step 2: Create a new column \"label\" to distinguish between train and test samples\n",
    "combined_df = combined_df.with_columns(\n",
    "    label=pl.when(pl.col('id')<=3000887)\n",
    "    .then(pl.lit('train'))\n",
    "    .otherwise(pl.lit('test'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map day of the week integers to day names\n",
    "day_names = list(calendar.day_name)\n",
    "\n",
    "day_name_dict = {key+1:value for key, value in enumerate(day_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3_029_400, 9)\n",
      "┌─────────┬────────────┬───────────┬────────────────────────────┬───────┬─────────────┬───────┬───────────┬─────────────┐\n",
      "│ id      ┆ date       ┆ store_nbr ┆ family                     ┆ sales ┆ onpromotion ┆ label ┆ dayofweek ┆ working_day │\n",
      "│ ---     ┆ ---        ┆ ---       ┆ ---                        ┆ ---   ┆ ---         ┆ ---   ┆ ---       ┆ ---         │\n",
      "│ i64     ┆ date       ┆ str       ┆ str                        ┆ f64   ┆ i64         ┆ str   ┆ str       ┆ str         │\n",
      "╞═════════╪════════════╪═══════════╪════════════════════════════╪═══════╪═════════════╪═══════╪═══════════╪═════════════╡\n",
      "│ 0       ┆ 2013-01-01 ┆ 1         ┆ AUTOMOTIVE                 ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         │\n",
      "│ 1       ┆ 2013-01-01 ┆ 1         ┆ BABY CARE                  ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         │\n",
      "│ 2       ┆ 2013-01-01 ┆ 1         ┆ BEAUTY                     ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         │\n",
      "│ 3       ┆ 2013-01-01 ┆ 1         ┆ BEVERAGES                  ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         │\n",
      "│ 4       ┆ 2013-01-01 ┆ 1         ┆ BOOKS                      ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         │\n",
      "│ …       ┆ …          ┆ …         ┆ …                          ┆ …     ┆ …           ┆ …     ┆ …         ┆ …           │\n",
      "│ 3029395 ┆ 2017-08-31 ┆ 9         ┆ POULTRY                    ┆ null  ┆ 1           ┆ test  ┆ Thursday  ┆ Yes         │\n",
      "│ 3029396 ┆ 2017-08-31 ┆ 9         ┆ PREPARED FOODS             ┆ null  ┆ 0           ┆ test  ┆ Thursday  ┆ Yes         │\n",
      "│ 3029397 ┆ 2017-08-31 ┆ 9         ┆ PRODUCE                    ┆ null  ┆ 1           ┆ test  ┆ Thursday  ┆ Yes         │\n",
      "│ 3029398 ┆ 2017-08-31 ┆ 9         ┆ SCHOOL AND OFFICE SUPPLIES ┆ null  ┆ 9           ┆ test  ┆ Thursday  ┆ Yes         │\n",
      "│ 3029399 ┆ 2017-08-31 ┆ 9         ┆ SEAFOOD                    ┆ null  ┆ 0           ┆ test  ┆ Thursday  ┆ Yes         │\n",
      "└─────────┴────────────┴───────────┴────────────────────────────┴───────┴─────────────┴───────┴───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df.with_columns(\n",
    "    dayofweek=pl.col('date').dt.weekday().cast(pl.Int8)\n",
    "    .replace(day_name_dict, default=None)\n",
    ")\n",
    "\n",
    "weekday = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "weekend = ['Saturday', 'Sunday']\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    working_day=pl.when(pl.col('dayofweek').is_in(weekday))\n",
    "    .then(pl.lit('Yes'))\n",
    "    .otherwise(pl.lit('No'))\n",
    ")\n",
    "\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Oil Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_218, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ date       ┆ oil_price │\n",
      "│ ---        ┆ ---       │\n",
      "│ date       ┆ f64       │\n",
      "╞════════════╪═══════════╡\n",
      "│ 2013-01-01 ┆ null      │\n",
      "│ 2013-01-02 ┆ 93.14     │\n",
      "│ 2013-01-03 ┆ 92.97     │\n",
      "│ 2013-01-04 ┆ 93.12     │\n",
      "│ 2013-01-07 ┆ 93.2      │\n",
      "│ …          ┆ …         │\n",
      "│ 2017-08-25 ┆ 47.65     │\n",
      "│ 2017-08-28 ┆ 46.4      │\n",
      "│ 2017-08-29 ┆ 46.46     │\n",
      "│ 2017-08-30 ┆ 45.96     │\n",
      "│ 2017-08-31 ┆ 47.26     │\n",
      "└────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oil_df = pl.read_csv('../data/input/oil.csv')\n",
    "oil_df = oil_df.with_columns(pl.col('date').str.strptime(pl.Date, '%m/%d/%y'))\n",
    "oil_df = oil_df.rename({'dcoilwtico': 'oil_price'})\n",
    "print(oil_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3_029_400, 10)\n",
      "┌─────────┬────────────┬───────────┬────────────────────────────┬───────┬─────────────┬───────┬───────────┬─────────────┬───────────┐\n",
      "│ id      ┆ date       ┆ store_nbr ┆ family                     ┆ sales ┆ onpromotion ┆ label ┆ dayofweek ┆ working_day ┆ oil_price │\n",
      "│ ---     ┆ ---        ┆ ---       ┆ ---                        ┆ ---   ┆ ---         ┆ ---   ┆ ---       ┆ ---         ┆ ---       │\n",
      "│ i64     ┆ date       ┆ str       ┆ str                        ┆ f64   ┆ i64         ┆ str   ┆ str       ┆ str         ┆ f64       │\n",
      "╞═════════╪════════════╪═══════════╪════════════════════════════╪═══════╪═════════════╪═══════╪═══════════╪═════════════╪═══════════╡\n",
      "│ 0       ┆ 2013-01-01 ┆ 1         ┆ AUTOMOTIVE                 ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         ┆ null      │\n",
      "│ 1       ┆ 2013-01-01 ┆ 1         ┆ BABY CARE                  ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         ┆ null      │\n",
      "│ 2       ┆ 2013-01-01 ┆ 1         ┆ BEAUTY                     ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         ┆ null      │\n",
      "│ 3       ┆ 2013-01-01 ┆ 1         ┆ BEVERAGES                  ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         ┆ null      │\n",
      "│ 4       ┆ 2013-01-01 ┆ 1         ┆ BOOKS                      ┆ 0.0   ┆ 0           ┆ train ┆ Tuesday   ┆ Yes         ┆ null      │\n",
      "│ …       ┆ …          ┆ …         ┆ …                          ┆ …     ┆ …           ┆ …     ┆ …         ┆ …           ┆ …         │\n",
      "│ 3029395 ┆ 2017-08-31 ┆ 9         ┆ POULTRY                    ┆ null  ┆ 1           ┆ test  ┆ Thursday  ┆ Yes         ┆ 47.26     │\n",
      "│ 3029396 ┆ 2017-08-31 ┆ 9         ┆ PREPARED FOODS             ┆ null  ┆ 0           ┆ test  ┆ Thursday  ┆ Yes         ┆ 47.26     │\n",
      "│ 3029397 ┆ 2017-08-31 ┆ 9         ┆ PRODUCE                    ┆ null  ┆ 1           ┆ test  ┆ Thursday  ┆ Yes         ┆ 47.26     │\n",
      "│ 3029398 ┆ 2017-08-31 ┆ 9         ┆ SCHOOL AND OFFICE SUPPLIES ┆ null  ┆ 9           ┆ test  ┆ Thursday  ┆ Yes         ┆ 47.26     │\n",
      "│ 3029399 ┆ 2017-08-31 ┆ 9         ┆ SEAFOOD                    ┆ null  ┆ 0           ┆ test  ┆ Thursday  ┆ Yes         ┆ 47.26     │\n",
      "└─────────┴────────────┴───────────┴────────────────────────────┴───────┴─────────────┴───────┴───────────┴─────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df.join(oil_df, how='left', on='date', validate='m:1')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df =combined_df.with_columns(pl.col('oil_price').fill_null(strategy='forward')\n",
    "                            .fill_null(strategy='backward')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing stores data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (54, 5)\n",
      "┌───────────┬───────────────┬────────────────────────────────┬──────┬─────────┐\n",
      "│ store_nbr ┆ city          ┆ state                          ┆ type ┆ cluster │\n",
      "│ ---       ┆ ---           ┆ ---                            ┆ ---  ┆ ---     │\n",
      "│ str       ┆ str           ┆ str                            ┆ str  ┆ str     │\n",
      "╞═══════════╪═══════════════╪════════════════════════════════╪══════╪═════════╡\n",
      "│ 1         ┆ Quito         ┆ Pichincha                      ┆ D    ┆ 13      │\n",
      "│ 2         ┆ Quito         ┆ Pichincha                      ┆ D    ┆ 13      │\n",
      "│ 3         ┆ Quito         ┆ Pichincha                      ┆ D    ┆ 8       │\n",
      "│ 4         ┆ Quito         ┆ Pichincha                      ┆ D    ┆ 9       │\n",
      "│ 5         ┆ Santo Domingo ┆ Santo Domingo de los Tsachilas ┆ D    ┆ 4       │\n",
      "│ …         ┆ …             ┆ …                              ┆ …    ┆ …       │\n",
      "│ 50        ┆ Ambato        ┆ Tungurahua                     ┆ A    ┆ 14      │\n",
      "│ 51        ┆ Guayaquil     ┆ Guayas                         ┆ A    ┆ 17      │\n",
      "│ 52        ┆ Manta         ┆ Manabi                         ┆ A    ┆ 11      │\n",
      "│ 53        ┆ Manta         ┆ Manabi                         ┆ D    ┆ 13      │\n",
      "│ 54        ┆ El Carmen     ┆ Manabi                         ┆ C    ┆ 3       │\n",
      "└───────────┴───────────────┴────────────────────────────────┴──────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "stores_df = pl.read_csv('../data/input/stores.csv',  dtypes={'store_nbr': str, 'cluster': str})\n",
    "print(stores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3_029_400, 14)\n",
      "┌─────────┬────────────┬───────────┬────────────────────────────┬───────┬─────────────┬───┬─────────────┬───────────┬───────┬───────────┬──────┬─────────┐\n",
      "│ id      ┆ date       ┆ store_nbr ┆ family                     ┆ sales ┆ onpromotion ┆ … ┆ working_day ┆ oil_price ┆ city  ┆ state     ┆ type ┆ cluster │\n",
      "│ ---     ┆ ---        ┆ ---       ┆ ---                        ┆ ---   ┆ ---         ┆   ┆ ---         ┆ ---       ┆ ---   ┆ ---       ┆ ---  ┆ ---     │\n",
      "│ i64     ┆ date       ┆ str       ┆ str                        ┆ f64   ┆ i64         ┆   ┆ str         ┆ f64       ┆ str   ┆ str       ┆ str  ┆ str     │\n",
      "╞═════════╪════════════╪═══════════╪════════════════════════════╪═══════╪═════════════╪═══╪═════════════╪═══════════╪═══════╪═══════════╪══════╪═════════╡\n",
      "│ 0       ┆ 2013-01-01 ┆ 1         ┆ AUTOMOTIVE                 ┆ 0.0   ┆ 0           ┆ … ┆ Yes         ┆ 93.14     ┆ Quito ┆ Pichincha ┆ D    ┆ 13      │\n",
      "│ 1       ┆ 2013-01-01 ┆ 1         ┆ BABY CARE                  ┆ 0.0   ┆ 0           ┆ … ┆ Yes         ┆ 93.14     ┆ Quito ┆ Pichincha ┆ D    ┆ 13      │\n",
      "│ 2       ┆ 2013-01-01 ┆ 1         ┆ BEAUTY                     ┆ 0.0   ┆ 0           ┆ … ┆ Yes         ┆ 93.14     ┆ Quito ┆ Pichincha ┆ D    ┆ 13      │\n",
      "│ 3       ┆ 2013-01-01 ┆ 1         ┆ BEVERAGES                  ┆ 0.0   ┆ 0           ┆ … ┆ Yes         ┆ 93.14     ┆ Quito ┆ Pichincha ┆ D    ┆ 13      │\n",
      "│ 4       ┆ 2013-01-01 ┆ 1         ┆ BOOKS                      ┆ 0.0   ┆ 0           ┆ … ┆ Yes         ┆ 93.14     ┆ Quito ┆ Pichincha ┆ D    ┆ 13      │\n",
      "│ …       ┆ …          ┆ …         ┆ …                          ┆ …     ┆ …           ┆ … ┆ …           ┆ …         ┆ …     ┆ …         ┆ …    ┆ …       │\n",
      "│ 3029395 ┆ 2017-08-31 ┆ 9         ┆ POULTRY                    ┆ null  ┆ 1           ┆ … ┆ Yes         ┆ 47.26     ┆ Quito ┆ Pichincha ┆ B    ┆ 6       │\n",
      "│ 3029396 ┆ 2017-08-31 ┆ 9         ┆ PREPARED FOODS             ┆ null  ┆ 0           ┆ … ┆ Yes         ┆ 47.26     ┆ Quito ┆ Pichincha ┆ B    ┆ 6       │\n",
      "│ 3029397 ┆ 2017-08-31 ┆ 9         ┆ PRODUCE                    ┆ null  ┆ 1           ┆ … ┆ Yes         ┆ 47.26     ┆ Quito ┆ Pichincha ┆ B    ┆ 6       │\n",
      "│ 3029398 ┆ 2017-08-31 ┆ 9         ┆ SCHOOL AND OFFICE SUPPLIES ┆ null  ┆ 9           ┆ … ┆ Yes         ┆ 47.26     ┆ Quito ┆ Pichincha ┆ B    ┆ 6       │\n",
      "│ 3029399 ┆ 2017-08-31 ┆ 9         ┆ SEAFOOD                    ┆ null  ┆ 0           ┆ … ┆ Yes         ┆ 47.26     ┆ Quito ┆ Pichincha ┆ B    ┆ 6       │\n",
      "└─────────┴────────────┴───────────┴────────────────────────────┴───────┴─────────────┴───┴─────────────┴───────────┴───────┴───────────┴──────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df.join(stores_df, how='left', on='store_nbr', validate='m:1')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Holiday-Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (350, 10)\n",
      "┌────────────┬────────────┬──────────┬─────────────┬───────────────────────────────┬─────────────┬───────────┬─────────────┬─────────────────────┬─────────────────────┐\n",
      "│ date       ┆ type       ┆ locale   ┆ locale_name ┆ description                   ┆ transferred ┆ dayofweek ┆ working_day ┆ description_base_es ┆ description_base_en │\n",
      "│ ---        ┆ ---        ┆ ---      ┆ ---         ┆ ---                           ┆ ---         ┆ ---       ┆ ---         ┆ ---                 ┆ ---                 │\n",
      "│ date       ┆ str        ┆ str      ┆ str         ┆ str                           ┆ bool        ┆ str       ┆ str         ┆ str                 ┆ str                 │\n",
      "╞════════════╪════════════╪══════════╪═════════════╪═══════════════════════════════╪═════════════╪═══════════╪═════════════╪═════════════════════╪═════════════════════╡\n",
      "│ 2012-03-02 ┆ Holiday    ┆ Local    ┆ Manta       ┆ Fundacion de Manta            ┆ false       ┆ Friday    ┆ No          ┆ Fundacion           ┆ Foundation          │\n",
      "│ 2012-04-01 ┆ Holiday    ┆ Regional ┆ Cotopaxi    ┆ Provincializacion de Cotopaxi ┆ false       ┆ Sunday    ┆ No          ┆ Provincializacion   ┆ Provincialization   │\n",
      "│ 2012-04-12 ┆ Holiday    ┆ Local    ┆ Cuenca      ┆ Fundacion de Cuenca           ┆ false       ┆ Thursday  ┆ No          ┆ Fundacion           ┆ Foundation          │\n",
      "│ 2012-04-14 ┆ Holiday    ┆ Local    ┆ Libertad    ┆ Cantonizacion de Libertad     ┆ false       ┆ Saturday  ┆ No          ┆ Cantonizacion       ┆ Cantonization       │\n",
      "│ 2012-04-21 ┆ Holiday    ┆ Local    ┆ Riobamba    ┆ Cantonizacion de Riobamba     ┆ false       ┆ Saturday  ┆ No          ┆ Cantonizacion       ┆ Cantonization       │\n",
      "│ …          ┆ …          ┆ …        ┆ …           ┆ …                             ┆ …           ┆ …         ┆ …           ┆ …                   ┆ …                   │\n",
      "│ 2017-12-22 ┆ Additional ┆ National ┆ Ecuador     ┆ Navidad-3                     ┆ false       ┆ Friday    ┆ No          ┆ Navidad             ┆ Christmas           │\n",
      "│ 2017-12-23 ┆ Additional ┆ National ┆ Ecuador     ┆ Navidad-2                     ┆ false       ┆ Saturday  ┆ No          ┆ Navidad             ┆ Christmas           │\n",
      "│ 2017-12-24 ┆ Additional ┆ National ┆ Ecuador     ┆ Navidad-1                     ┆ false       ┆ Sunday    ┆ No          ┆ Navidad             ┆ Christmas           │\n",
      "│ 2017-12-25 ┆ Holiday    ┆ National ┆ Ecuador     ┆ Navidad                       ┆ false       ┆ Monday    ┆ No          ┆ Navidad             ┆ Christmas           │\n",
      "│ 2017-12-26 ┆ Additional ┆ National ┆ Ecuador     ┆ Navidad+1                     ┆ false       ┆ Tuesday   ┆ No          ┆ Navidad             ┆ Christmas           │\n",
      "└────────────┴────────────┴──────────┴─────────────┴───────────────────────────────┴─────────────┴───────────┴─────────────┴─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "holiday_events_df = pl.read_csv('../data/input/holidays_events_mod_pl.csv', try_parse_dates=True)\n",
    "print(holiday_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (343, 10)\n",
      "┌────────────┬────────────┬──────────┬─────────────┬───────────────────────────────────┬─────────────┬───────────┬─────────────┬─────────────────────────────┬────────────────────────┐\n",
      "│ date       ┆ type       ┆ locale   ┆ locale_name ┆ description                       ┆ transferred ┆ dayofweek ┆ working_day ┆ description_base_es         ┆ description_base_en    │\n",
      "│ ---        ┆ ---        ┆ ---      ┆ ---         ┆ ---                               ┆ ---         ┆ ---       ┆ ---         ┆ ---                         ┆ ---                    │\n",
      "│ date       ┆ str        ┆ str      ┆ str         ┆ str                               ┆ bool        ┆ str       ┆ str         ┆ str                         ┆ str                    │\n",
      "╞════════════╪════════════╪══════════╪═════════════╪═══════════════════════════════════╪═════════════╪═══════════╪═════════════╪═════════════════════════════╪════════════════════════╡\n",
      "│ 2012-08-10 ┆ Holiday    ┆ National ┆ Ecuador     ┆ Primer Grito de Independencia     ┆ false       ┆ Friday    ┆ No          ┆ Primer Grito  Independencia ┆ First Cry Independence │\n",
      "│ 2012-10-09 ┆ Holiday    ┆ National ┆ Ecuador     ┆ Independencia de Guayaquil        ┆ true        ┆ Tuesday   ┆ No          ┆ Independencia               ┆ Independence           │\n",
      "│ 2012-10-12 ┆ Transfer   ┆ National ┆ Ecuador     ┆ Traslado Independencia de Guayaq… ┆ false       ┆ Friday    ┆ No          ┆  Independencia              ┆ Independence           │\n",
      "│ 2012-11-02 ┆ Holiday    ┆ National ┆ Ecuador     ┆ Dia de Difuntos                   ┆ false       ┆ Friday    ┆ No          ┆ Dia Difuntos                ┆ Day of the Dead        │\n",
      "│ 2012-11-03 ┆ Holiday    ┆ National ┆ Ecuador     ┆ Independencia de Cuenca           ┆ false       ┆ Saturday  ┆ No          ┆ Independencia               ┆ Independence           │\n",
      "│ …          ┆ …          ┆ …        ┆ …           ┆ …                                 ┆ …           ┆ …         ┆ …           ┆ …                           ┆ …                      │\n",
      "│ 2017-12-05 ┆ Additional ┆ Local    ┆ Quito       ┆ Fundacion de Quito-1              ┆ false       ┆ Tuesday   ┆ No          ┆ Fundacion                   ┆ Foundation             │\n",
      "│ 2017-12-06 ┆ Holiday    ┆ Local    ┆ Quito       ┆ Fundacion de Quito                ┆ true        ┆ Wednesday ┆ No          ┆ Fundacion                   ┆ Foundation             │\n",
      "│ 2017-12-08 ┆ Holiday    ┆ Local    ┆ Loja        ┆ Fundacion de Loja                 ┆ false       ┆ Friday    ┆ No          ┆ Fundacion                   ┆ Foundation             │\n",
      "│ 2017-12-08 ┆ Transfer   ┆ Local    ┆ Quito       ┆ Traslado Fundacion de Quito       ┆ false       ┆ Friday    ┆ No          ┆  Fundacion                  ┆ Foundation             │\n",
      "│ 2017-12-22 ┆ Holiday    ┆ Local    ┆ Salinas     ┆ Cantonizacion de Salinas          ┆ false       ┆ Friday    ┆ No          ┆ Cantonizacion               ┆ Cantonization          │\n",
      "└────────────┴────────────┴──────────┴─────────────┴───────────────────────────────────┴─────────────┴───────────┴─────────────┴─────────────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "type1_unique = holiday_events_df.filter(pl.col('locale')=='National').unique(subset=['date'], maintain_order=True)\n",
    "type2_unique = holiday_events_df.filter(pl.col('locale')!='National').unique(subset=['date', 'locale_name'], maintain_order=True)\n",
    "\n",
    "# Combine both types of duplicates into a single boolean Series\n",
    "holiday_events_df = pl.concat([type1_unique, type2_unique])\n",
    "\n",
    "print(holiday_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First condition (when locale=='National')\n",
    "combined_df = combined_df.join(holiday_events_df.filter(pl.col('locale')=='National'), how='left', left_on='date', right_on='date', suffix='_national')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second condition (when locale!='National')\n",
    "combined_df = combined_df.join(holiday_events_df.filter(pl.col('locale')!='National'), how='left', left_on=['date', 'state'], right_on=['date', 'locale_name'], suffix='_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "test_df = []\n",
    "oil_df= []\n",
    "stores_df = []\n",
    "# holiday_events_df = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.with_columns(\n",
    "    working_day_final=pl.when((pl.col('working_day_national').is_not_null()) & (pl.col('working_day_state').is_null()))\n",
    "    .then(pl.col('working_day_national'))\n",
    "    .when((pl.col('working_day_national').is_null()) & (pl.col('working_day_state').is_not_null()))\n",
    "    .then(pl.col('working_day_state'))\n",
    "    .when((pl.col('working_day_national').is_not_null()) & (pl.col('working_day_state').is_not_null()) & (pl.col('state')==pl.col('locale_name')))\n",
    "    .then(pl.col('working_day_state'))\n",
    "    .when((pl.col('working_day_national').is_not_null()) & (pl.col('working_day_state').is_not_null()) & (pl.col('state')!=pl.col('locale_name')))\n",
    "    .then(pl.col('working_day_national'))\n",
    "    .otherwise(pl.col('working_day'))\n",
    ")\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    type_of_day=pl.when((pl.col('type_national').is_not_null()) & (pl.col('type_state').is_null()))\n",
    "    .then(pl.col('type_national'))\n",
    "    .when((pl.col('type_national').is_null()) & (pl.col('type_state').is_not_null()))\n",
    "    .then(pl.col('type_state'))\n",
    "    .when((pl.col('type_national').is_not_null()) & (pl.col('type_state').is_not_null()) & (pl.col('state')==pl.col('locale_name')))\n",
    "    .then(pl.col('type_state'))\n",
    "    .when((pl.col('type_national').is_not_null()) & (pl.col('type_state').is_not_null()) & (pl.col('state')!=pl.col('locale_name')))\n",
    "    .then(pl.col('type_national'))\n",
    "    .otherwise(pl.lit('regular_day'))\n",
    ")\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    description_final=pl.when((pl.col('description_base_en').is_not_null()) & (pl.col('description_base_en_state').is_null()))\n",
    "    .then(pl.col('description_base_en'))\n",
    "    .when((pl.col('description_base_en').is_null()) & (pl.col('description_base_en_state').is_not_null()))\n",
    "    .then(pl.col('description_base_en_state'))\n",
    "    .when((pl.col('description_base_en').is_not_null()) & (pl.col('description_base_en_state').is_not_null()) & (pl.col('state')==pl.col('locale_name')))\n",
    "    .then(pl.col('description_base_en_state'))\n",
    "    .when((pl.col('description_base_en').is_not_null()) & (pl.col('description_base_en_state').is_not_null()) & (pl.col('state')!=pl.col('locale_name')))\n",
    "    .then(pl.col('description_base_en'))\n",
    "    .otherwise(pl.lit('regular_day'))\n",
    ")\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    locale_name_final=pl.when((pl.col('locale_name').is_not_null()) & (pl.col('locale_state').is_null()))\n",
    "    .then(pl.lit('national'))\n",
    "    .when((pl.col('locale_name').is_null()) & (pl.col('locale_state').is_not_null()))\n",
    "    .then(pl.lit('state'))\n",
    "    .when((pl.col('locale_name').is_not_null()) & (pl.col('locale_state').is_not_null()))\n",
    "    .then(pl.lit('national+state'))\n",
    "    .otherwise(pl.lit('state'))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.with_columns(\n",
    "    type_weekday=pl.when(pl.col('dayofweek').is_in(weekday))\n",
    "    .then(pl.lit('weekday'))\n",
    "    .otherwise(pl.lit('weekend'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.with_columns(\n",
    "    # is_salary_day=pl.when((pl.col('date').dt.day().cast(pl.Int32) == pl.lit(15).cast(pl.Int32)) | (pl.col('date').dt.day() == pl.col('date').dt.month_end().dt.day()))\n",
    "    is_salary_day=pl.when((pl.col('date').dt.day() == 15) | (pl.col('date').dt.day() == pl.col('date').dt.month_end().dt.day()))\n",
    "    .then(pl.lit('Yes'))\n",
    "    .otherwise(pl.lit('No'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.with_columns(\n",
    "    month=pl.col('date').dt.strftime(\"%B\"),\n",
    "    year=pl.col('date').dt.strftime(\"%Y\"),\n",
    "    day=pl.col('date').dt.strftime(\"%d\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (40, 2)\n",
      "┌───────────────┬─────────────┐\n",
      "│ column        ┆ column_0    │\n",
      "│ ---           ┆ ---         │\n",
      "│ str           ┆ str         │\n",
      "╞═══════════════╪═════════════╡\n",
      "│ id            ┆ 244635      │\n",
      "│ date          ┆ 2013-05-18  │\n",
      "│ store_nbr     ┆ 23          │\n",
      "│ family        ┆ CELEBRATION │\n",
      "│ sales         ┆ 0.0         │\n",
      "│ …             ┆ …           │\n",
      "│ type_weekday  ┆ weekend     │\n",
      "│ is_salary_day ┆ No          │\n",
      "│ month         ┆ May         │\n",
      "│ year          ┆ 2013        │\n",
      "│ day           ┆ 18          │\n",
      "└───────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_number = random.randint(0, 3e6)\n",
    "\n",
    "print(combined_df.filter(\n",
    "    pl.col('id')==random_number\n",
    "    ).transpose(include_header = True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  { # flag to drop, type of variable\n",
    "'id'\t:\t[\t'Yes'\t,\t'id'\t]\t,\n",
    "'date'\t:\t[\t'No'\t,\t'date'\t]\t,\n",
    "'store_nbr'\t:\t[\t'No'\t,\t'store_nbr'\t]\t,\n",
    "'family'\t:\t[\t'No'\t,\t'family'\t]\t,\n",
    "'sales'\t:\t[\t'No'\t,\t'target'\t]\t,\n",
    "'onpromotion'\t:\t[\t'No'\t,\t'numerical'\t]\t,\n",
    "'label'\t:\t[\t'No'\t,      \t'label'\t]\t,\n",
    "'dayofweek'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'working_day'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'oil_price'\t:\t[\t'No'\t,\t'numerical'\t]\t,\n",
    "'city'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'type'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'cluster'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'type_national'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'locale'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'locale_name'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'description'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'transferred'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'dayofweek_national'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'working_day_national'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'description_base_es'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'description_base_en'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'type_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'locale_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "# 'locale_name_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,       # this column does not exist in polars version of combined_df\n",
    "'description_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'transferred_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'dayofweek_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'working_day_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'description_base_es_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'description_base_en_state'\t:\t[\t'Yes'\t,\t'categorical'\t]\t,\n",
    "'working_day_final'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'type_of_day'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'description_final'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'locale_name_final'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'type_weekday'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'is_salary_day'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'month'\t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'year' \t:\t[\t'No'\t,\t'categorical'\t]\t,\n",
    "'day'\t:\t[\t'No'\t,\t'categorical'\t]\t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(features):\n",
    "    \"\"\"Get columns to drop, numerical columns, and categorical columns based on feature dictionary.\"\"\"\n",
    "    drop_cols = [col for col, val in features.items() if val[0] == 'Yes']\n",
    "    num_cols = [col for col, val in features.items() if val[1] == 'numerical' and val[0] == 'No']\n",
    "    cat_cols = [col for col, val in features.items() if val[1] == 'categorical' and val[0] == 'No']\n",
    "    return drop_cols, num_cols, cat_cols\n",
    "\n",
    "# Get columns based on feature dictionary\n",
    "drop_cols, num_cols, cat_cols = get_columns(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.to_pandas()\n",
    "combined_df = combined_df.sort_values(by='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'store_nbr', 'family', 'sales', 'onpromotion', 'label',\n",
       "       'dayofweek', 'oil_price', 'type', 'cluster', 'working_day_final',\n",
       "       'type_of_day', 'description_final', 'locale_name_final', 'type_weekday',\n",
       "       'is_salary_day', 'month', 'year', 'day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DataFrame into training and test sets based on the 'label' column\n",
    "train_df = combined_df[combined_df['label'] == 'train']\n",
    "test_df = combined_df[combined_df['label'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_sales_families(df):\n",
    "    recent_date = df['date'].max() - timedelta(days=15)\n",
    "    zero_sales = df[df['date'] > recent_date].groupby('family')['sales'].sum() == 0\n",
    "    return zero_sales[zero_sales].index.tolist()\n",
    "\n",
    "zero_sales_families = get_zero_sales_families(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the categorical and numerical features\n",
    "# categorical_features = ['store_nbr', 'dayofweek', 'type', 'cluster', \n",
    "#                         'working_day_final', 'type_of_day', 'description_final', \n",
    "#                         'locale_name_final', 'type_weekday', 'is_salary_day', 'month', 'year', 'day']\n",
    "# numerical_features = ['onpromotion', 'oil_price']\n",
    "\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                            ('scaler', StandardScaler())]), num_cols),\n",
    "    ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                            ('encoder', OneHotEncoder(handle_unknown='ignore'))]), cat_cols),\n",
    "])\n",
    "\n",
    "# Model definition\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(objective='reg:squaredlogerror', n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(features):\n",
    "    \"\"\"Get columns to drop, numerical columns, and categorical columns based on feature dictionary.\"\"\"\n",
    "    drop_cols = [col for col, val in features.items() if val[0] == 'Yes']\n",
    "    num_cols = [col for col, val in features.items() if val[1] == 'numerical' and val[0] == 'No']\n",
    "    cat_cols = [col for col, val in features.items() if val[1] == 'categorical' and val[0] == 'No']\n",
    "    return drop_cols, num_cols, cat_cols\n",
    "\n",
    "# Get columns based on feature dictionary\n",
    "drop_cols, num_cols, cat_cols = get_columns(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Root Mean Squared Log Error for two arrays.\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred), \"Mismatched length between true and predicted values\"\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_time_series_cv_rmsle(model, X, y, dates, n_splits):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation, ensuring no leakage between training and test sets.\n",
    "    \n",
    "    :param model: The model to be trained and validated.\n",
    "    :param X: Features DataFrame.\n",
    "    :param y: Target variable Series.\n",
    "    :param dates: Series or array containing the date for each observation in X.\n",
    "    :param n_splits: Number of splits for cross-validation.\n",
    "    :return: List of RMSLE scores for each fold.\n",
    "    \"\"\"\n",
    "    rmsle_scores = []\n",
    "    unique_dates = np.array(sorted(dates.unique()))\n",
    "    split_indices = np.array_split(unique_dates, n_splits + 1)\n",
    "    \n",
    "    for i in range(1, len(split_indices)):\n",
    "        train_dates = np.concatenate(split_indices[:i])\n",
    "        test_dates = split_indices[i]\n",
    "        \n",
    "        train_indices = dates[dates.isin(train_dates)].index\n",
    "        test_indices = dates[dates.isin(test_dates)].index\n",
    "        \n",
    "        X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "        y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        predictions = np.clip(predictions, a_min=0, a_max=None)  # Ensure predictions are non-negative\n",
    "        rmsle_score = rmsle(y_test, predictions)\n",
    "        rmsle_scores.append(rmsle_score)\n",
    "    \n",
    "    return rmsle_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    # Replace any character that is not alphanumeric or an underscore with an underscore\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', filename)\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'model__n_estimators': [100, 500, 1000],\n",
    "    'model__max_depth': [3, 6, 9],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 1.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for AUTOMOTIVE already exists. Skipping training.\n",
      "Model for CELEBRATION already exists. Skipping training.\n",
      "Training model for BREAD/BAKERY.\n",
      "Best RMSLE for BREAD/BAKERY: 1.43 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 0.7}\n",
      "Training model for BOOKS.\n",
      "Best RMSLE for BOOKS: 0.14 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__subsample': 0.7}\n",
      "Training model for BEVERAGES.\n",
      "Best RMSLE for BEVERAGES: 1.86 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.3, 'model__max_depth': 6, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for BEAUTY.\n",
      "Best RMSLE for BEAUTY: 0.65 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for BABY CARE.\n",
      "Best RMSLE for BABY CARE: 0.26 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 500, 'model__subsample': 1.0}\n",
      "Training model for SEAFOOD.\n",
      "Best RMSLE for SEAFOOD: 0.94 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for SCHOOL AND OFFICE SUPPLIES.\n",
      "Best RMSLE for SCHOOL AND OFFICE SUPPLIES: 0.73 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.3, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__subsample': 0.7}\n",
      "Training model for PRODUCE.\n",
      "Best RMSLE for PRODUCE: 2.41 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.3, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__subsample': 0.7}\n",
      "Training model for PREPARED FOODS.\n",
      "Best RMSLE for PREPARED FOODS: 1.11 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for POULTRY.\n",
      "Best RMSLE for POULTRY: 1.46 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.3, 'model__max_depth': 6, 'model__n_estimators': 1000, 'model__subsample': 0.7}\n",
      "Training model for PLAYERS AND ELECTRONICS.\n",
      "Best RMSLE for PLAYERS AND ELECTRONICS: 1.34 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 0.7}\n",
      "Training model for PET SUPPLIES.\n",
      "Best RMSLE for PET SUPPLIES: 1.04 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 6, 'model__n_estimators': 500, 'model__subsample': 0.8}\n",
      "Training model for PERSONAL CARE.\n",
      "Best RMSLE for PERSONAL CARE: 1.21 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for MEATS.\n",
      "Best RMSLE for MEATS: 1.36 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for MAGAZINES.\n",
      "Best RMSLE for MAGAZINES: 1.03 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 100, 'model__subsample': 0.7}\n",
      "Training model for LIQUOR,WINE,BEER.\n",
      "Best RMSLE for LIQUOR,WINE,BEER: 1.22 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.1, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for LINGERIE.\n",
      "Best RMSLE for LINGERIE: 0.86 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.01, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for LAWN AND GARDEN.\n",
      "Best RMSLE for LAWN AND GARDEN: 0.84 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.1, 'model__max_depth': 6, 'model__n_estimators': 500, 'model__subsample': 0.8}\n",
      "Training model for LADIESWEAR.\n",
      "Best RMSLE for LADIESWEAR: 1.32 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for HOME CARE.\n",
      "Best RMSLE for HOME CARE: 1.92 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.3, 'model__max_depth': 3, 'model__n_estimators': 500, 'model__subsample': 1.0}\n",
      "Training model for HOME APPLIANCES.\n",
      "Best RMSLE for HOME APPLIANCES: 0.41 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 500, 'model__subsample': 0.7}\n",
      "Training model for CLEANING.\n",
      "Best RMSLE for CLEANING: 1.53 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 0.8}\n",
      "Training model for DAIRY.\n",
      "Best RMSLE for DAIRY: 1.46 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 0.8}\n",
      "Training model for DELI.\n",
      "Best RMSLE for DELI: 1.29 with params {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 0.8}\n",
      "Training model for EGGS.\n",
      "Best RMSLE for EGGS: 1.27 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 500, 'model__subsample': 1.0}\n",
      "Training model for HOME AND KITCHEN II.\n",
      "Best RMSLE for HOME AND KITCHEN II: 1.01 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 6, 'model__n_estimators': 1000, 'model__subsample': 0.7}\n",
      "Training model for HOME AND KITCHEN I.\n",
      "Best RMSLE for HOME AND KITCHEN I: 1.11 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 0.8}\n",
      "Training model for HARDWARE.\n",
      "Best RMSLE for HARDWARE: 0.58 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.3, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__subsample': 0.7}\n",
      "Training model for GROCERY II.\n",
      "Best RMSLE for GROCERY II: 0.96 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 6, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for GROCERY I.\n",
      "Best RMSLE for GROCERY I: 1.95 with params {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.3, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n",
      "Training model for FROZEN FOODS.\n",
      "Best RMSLE for FROZEN FOODS: 1.19 with params {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.1, 'model__max_depth': 9, 'model__n_estimators': 1000, 'model__subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_scores = []\n",
    "family_models = {}\n",
    "\n",
    "\n",
    "model_directory = '../models/'  # Define the directory to store models\n",
    "os.makedirs(model_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "for family in train_df['family'].unique():\n",
    "    # Replace slashes in the model_identifier to prevent directory path issues\n",
    "    model_identifier = sanitize_filename(f'{family}')\n",
    "    model_path = os.path.join(model_directory, f'{model_identifier}_model.joblib')\n",
    "\n",
    "    \n",
    "    # Check if the model already exists\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model for {family} already exists. Skipping training.\")\n",
    "        family_model = load(model_path)  # Load the existing model\n",
    "    else:\n",
    "        print(f\"Training model for {family}.\")\n",
    "        family_df = train_df[train_df['family'] == family]\n",
    "        \n",
    "        X_family = family_df.drop(['sales', 'label', 'date', 'family'], axis=1)\n",
    "        y_family = family_df['sales']\n",
    "        dates_family = family_df['date']\n",
    "        \n",
    "        # Define and fit the model pipeline (including hyperparameter tuning if applicable)\n",
    "        family_model_pipeline = clone(model_pipeline)\n",
    "        \n",
    "        best_score = np.inf\n",
    "        best_params = None\n",
    "        \n",
    "        for params in ParameterGrid(hyperparameter_grid):\n",
    "            family_model_pipeline.set_params(**params)\n",
    "            avg_rmsle = np.mean(enhanced_time_series_cv_rmsle(family_model_pipeline, X_family, y_family, dates_family, n_splits=3))\n",
    "            \n",
    "            if avg_rmsle < best_score:\n",
    "                best_score = avg_rmsle\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"Best RMSLE for {family}: {round(best_score, 2)} with params {best_params}\")\n",
    "        \n",
    "        # Retrain the model on the entire dataset with the best parameters\n",
    "        family_model_pipeline.set_params(**best_params)\n",
    "        family_model_pipeline.fit(X_family, y_family)\n",
    "        \n",
    "        # Save the trained model\n",
    "        dump(family_model_pipeline, model_path)\n",
    "        \n",
    "        family_models[family] = family_model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_sales_families(df, cutoff_date):\n",
    "    recent_df = df[df['date'] > cutoff_date]\n",
    "    zero_sales_families = recent_df.groupby('family')['sales'].sum()\n",
    "    return zero_sales_families[zero_sales_families == 0].index.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date_train = train_df['date'].max()\n",
    "cutoff_date = max_date_train - pd.Timedelta(days=21)\n",
    "zero_sales_families = get_zero_sales_families(train_df, cutoff_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming zero_sales_families is a list of families with zero sales in the last period\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "for family in test_df['family'].unique():\n",
    "    # Copy the filtered test_df for the specific family\n",
    "    # and reset the index while keeping the index (which serves as 'id') in a column\n",
    "    family_test_df = test_df[test_df['family'] == family].copy().reset_index()\n",
    "    family_test_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "    # If this family should have zero sales, set predictions directly\n",
    "    if family in zero_sales_families:\n",
    "        family_test_df['sales'] = 0\n",
    "    else:\n",
    "        model_path = os.path.join(model_directory, f'{sanitize_filename(family)}_model.joblib')\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            family_model = load(model_path)\n",
    "        else:\n",
    "            print(f\"No pre-trained model found for {family}. Skipping.\")\n",
    "            continue  # Skip this family if no model is found\n",
    "        \n",
    "        X_test_family = family_test_df.drop(['sales', 'label', 'date', 'family', 'id'], axis=1)\n",
    "        \n",
    "        # Make predictions for families not in zero_sales_families\n",
    "        predictions = family_model.predict(X_test_family)\n",
    "        predictions = np.clip(predictions, a_min=0, a_max=None)  # Ensure non-negative predictions\n",
    "        family_test_df['sales'] = predictions\n",
    "    \n",
    "    # Append the predictions for the current family to the submission DataFrame\n",
    "    submission = pd.concat([submission, family_test_df[['id', 'sales']]], ignore_index=True)\n",
    "\n",
    "# Ensure 'id' is an integer for the submission file\n",
    "submission['id'] = submission['id'].astype(int)\n",
    "submission.to_csv('final_sales_predictions_family.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "store_sales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
